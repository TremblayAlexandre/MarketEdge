{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "execution_state": "idle",
   "id": "ccbc7bc8-4f94-4ffc-a2df-7df109b29dfa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T13:16:50.665233Z",
     "iopub.status.busy": "2025-11-02T13:16:50.664895Z",
     "iopub.status.idle": "2025-11-02T13:16:50.669082Z",
     "shell.execute_reply": "2025-11-02T13:16:50.668223Z",
     "shell.execute_reply.started": "2025-11-02T13:16:50.665211Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import glob\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import re\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm  # progress bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "execution_state": "idle",
   "id": "c0a52269-b52c-45b1-ac64-66641cff5c2c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T13:16:52.063966Z",
     "iopub.status.busy": "2025-11-02T13:16:52.063579Z",
     "iopub.status.idle": "2025-11-02T13:16:52.080153Z",
     "shell.execute_reply": "2025-11-02T13:16:52.079341Z",
     "shell.execute_reply.started": "2025-11-02T13:16:52.063934Z"
    }
   },
   "outputs": [],
   "source": [
    "def setup_finbert():\n",
    "    \"\"\"Initialize FinBERT model and tokenizer\"\"\"\n",
    "    print(\"ðŸ”¹ Loading FinBERT model...\")\n",
    "    model_name = \"ProsusAI/finbert\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "    model.eval()\n",
    "    return tokenizer, model\n",
    "\n",
    "\n",
    "def truncate_text_bytes(text, max_bytes=2000):\n",
    "    \"\"\"Ensure text fits under max_bytes (UTF-8 safe)\"\"\"\n",
    "    encoded = text.encode(\"utf-8\")\n",
    "    if len(encoded) <= max_bytes:\n",
    "        return text\n",
    "    truncated = encoded[:max_bytes]\n",
    "    # Ensure we don't cut off in the middle of a UTF-8 character\n",
    "    return truncated.decode(\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "\n",
    "def chunk_text(text, max_length=512):\n",
    "    \"\"\"Split text into chunks that fit FinBERT's token limit\"\"\"\n",
    "    sentences = re.split(r'[.!?]+', text)\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    for sentence in sentences:\n",
    "        if len(current_chunk) + len(sentence) < max_length * 4:\n",
    "            current_chunk += sentence + \". \"\n",
    "        else:\n",
    "            if current_chunk:\n",
    "                chunks.append(current_chunk.strip())\n",
    "            current_chunk = sentence + \". \"\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def analyze_with_finbert(text, tokenizer, model):\n",
    "    \"\"\"Analyze text with FinBERT and return sentiment summary\"\"\"\n",
    "    text = truncate_text_bytes(text, 2000)\n",
    "\n",
    "    if not text or len(text.strip()) < 10:\n",
    "        return {\n",
    "            \"sentiment\": \"neutral\",\n",
    "            \"confidence\": 0.0,\n",
    "            \"summary\": \"Insufficient text for analysis\"\n",
    "        }\n",
    "\n",
    "    chunks = chunk_text(text)\n",
    "    sentiments, confidences = [], []\n",
    "\n",
    "    for chunk in chunks[:10]:  # Limit to 10 chunks max\n",
    "        try:\n",
    "            inputs = tokenizer(chunk, return_tensors=\"pt\", truncation=True, max_length=512, padding=True)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "                predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "            sentiment_idx = torch.argmax(predictions, dim=-1).item()\n",
    "            confidence = torch.max(predictions).item()\n",
    "            sentiment_labels = [\"negative\", \"neutral\", \"positive\"]\n",
    "            sentiments.append(sentiment_labels[sentiment_idx])\n",
    "            confidences.append(confidence)\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    if not sentiments:\n",
    "        return {\n",
    "            \"sentiment\": \"neutral\",\n",
    "            \"confidence\": 0.0,\n",
    "            \"summary\": \"Analysis failed\"\n",
    "        }\n",
    "\n",
    "    sentiment_counts = {\"negative\": 0, \"neutral\": 0, \"positive\": 0}\n",
    "    for s in sentiments:\n",
    "        sentiment_counts[s] += 1\n",
    "\n",
    "    overall_sentiment = max(sentiment_counts, key=sentiment_counts.get)\n",
    "    avg_confidence = sum(confidences) / len(confidences)\n",
    "\n",
    "    summary = (\n",
    "        f\"FinBERT analysis on {len(chunks)} text segments shows {overall_sentiment.upper()} sentiment \"\n",
    "        f\"({avg_confidence:.2f} confidence). \"\n",
    "        f\"Distribution: {sentiment_counts['negative']} negative, {sentiment_counts['neutral']} neutral, \"\n",
    "        f\"{sentiment_counts['positive']} positive.\"\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"sentiment\": overall_sentiment,\n",
    "        \"confidence\": avg_confidence,\n",
    "        \"summary\": summary,\n",
    "        \"segment_count\": len(chunks),\n",
    "        \"sentiment_distribution\": sentiment_counts\n",
    "    }\n",
    "\n",
    "\n",
    "def process_file(file_path, tokenizer, model):\n",
    "    \"\"\"Process a single JSON file\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        part2item7_text = data.get('data', {}).get('sections', {}).get('part2item7', {}).get('text', '')\n",
    "        if not part2item7_text:\n",
    "            return False, \"no_part2item7\"\n",
    "\n",
    "        analysis = analyze_with_finbert(part2item7_text, tokenizer, model)\n",
    "\n",
    "        data['finberg'] = {\n",
    "            \"summary\": analysis['summary'],\n",
    "            \"sentiment\": analysis['sentiment'],\n",
    "            \"confidence\": analysis['confidence'],\n",
    "            \"segment_count\": analysis.get('segment_count', 0),\n",
    "            \"sentiment_distribution\": analysis.get('sentiment_distribution', {}),\n",
    "            \"generated_by\": \"finbert\",\n",
    "            \"source_section\": \"part2item7\",\n",
    "            \"ticker\": data.get('ticker', 'UNKNOWN'),\n",
    "            \"analysis_date\": datetime.now().isoformat()\n",
    "        }\n",
    "\n",
    "        with open(file_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "        return True, analysis['sentiment']\n",
    "\n",
    "    except Exception as e:\n",
    "        return False, str(e)\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to process all JSON files in completed_data_plus directory\"\"\"\n",
    "    data_dir = \"/home/sagemaker-user/shared/Project/advanced_tags/completed_data_plus\"\n",
    "    json_files = glob.glob(os.path.join(data_dir, \"*.json\"))\n",
    "    print(f\"ðŸ“‚ Found {len(json_files)} JSON files to process in {data_dir}\")\n",
    "\n",
    "    if not json_files:\n",
    "        print(f\"No JSON files found in {data_dir}.\")\n",
    "        return\n",
    "\n",
    "    tokenizer, model = setup_finbert()\n",
    "    successful, failed = 0, 0\n",
    "\n",
    "    for file_path in tqdm(json_files, desc=\"Processing files\", unit=\"file\"):\n",
    "        ok, result = process_file(file_path, tokenizer, model)\n",
    "        if ok:\n",
    "            successful += 1\n",
    "        else:\n",
    "            failed += 1\n",
    "\n",
    "    print(f\"\\nâœ… Processing complete: {successful} succeeded, âŒ {failed} failed.\")\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "execution_state": "idle",
   "id": "new-execution-cell",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T13:16:55.630576Z",
     "iopub.status.busy": "2025-11-02T13:16:55.630289Z",
     "iopub.status.idle": "2025-11-02T13:24:07.552343Z",
     "shell.execute_reply": "2025-11-02T13:24:07.551653Z",
     "shell.execute_reply.started": "2025-11-02T13:16:55.630550Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory exists: True\n",
      "Found 490 JSON files\n",
      "ðŸ“‚ Found 490 JSON files to process in /home/sagemaker-user/shared/Project/advanced_tags/completed_data_plus\n",
      "ðŸ”¹ Loading FinBERT model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2aa90e600fcc435f801b8a27d458ea4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/252 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d04ada3d7d54240a6e1d38d51374d14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/758 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7ca5979a1cc479b8ea5c43ef3fc97e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67718cf140004ed099d0383cc51a73cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-02 13:16:59.664498: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "181009f1b0734aa396ff622fb0c0f330",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:   0%|          | 0/490 [00:00<?, ?file/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fa97308610f4664bd6dcf80a97eafc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 490/490 [06:52<00:00,  1.19file/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Processing complete: 428 succeeded, âŒ 62 failed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Tester le chemin et exÃ©cuter l'analyse FinBERT\n",
    "import os\n",
    "data_dir = \"/home/sagemaker-user/shared/Project/advanced_tags/completed_data_plus\"\n",
    "print(f\"Directory exists: {os.path.exists(data_dir)}\")\n",
    "if os.path.exists(data_dir):\n",
    "    files = os.listdir(data_dir)\n",
    "    json_files = [f for f in files if f.endswith('.json')]\n",
    "    print(f\"Found {len(json_files)} JSON files\")\n",
    "    main()\n",
    "else:\n",
    "    print(\"Directory not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d75e95e-5acb-4e73-aac4-89a75ded76f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
